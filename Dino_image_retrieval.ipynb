{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0356c965",
   "metadata": {},
   "source": [
    "1. Imports and Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "DATA_DIR      = \"your_dataset_path\"\n",
    "TRAIN_DIR     = os.path.join(DATA_DIR, \"train\")\n",
    "QUERY_DIR     = os.path.join(DATA_DIR, \"test\", \"query\")\n",
    "GALLERY_DIR   = os.path.join(DATA_DIR, \"test\", \"gallery\")\n",
    "\n",
    "BATCH_SIZE    = 32\n",
    "NUM_EPOCHS    = 5\n",
    "LR            = 1e-4\n",
    "WEIGHT_DECAY  = 5e-4\n",
    "TOP_K         = 10\n",
    "EMB_SIZE      = 768  # for DINOv2-base embeddings\n",
    "VAL_SPLIT     = 0.2\n",
    "NUM_WORKERS   = 2\n",
    "\n",
    "# Automatically select GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === RANDOM SEED FOR REPRODUCIBILITY ===\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d2a10",
   "metadata": {},
   "source": [
    "2. Data Preparation: Transforms, Splits, Triplet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77767e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMAGE TRANSFORMATIONS ===\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(518),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Optional: More aggressive augmentation (disabled for now)\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(518),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "#     transforms.RandomGrayscale(p=0.1),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(540),\n",
    "    transforms.CenterCrop(518),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# === TRAIN/VALIDATION SPLIT (STRATIFIED) ===\n",
    "full_dataset = datasets.ImageFolder(TRAIN_DIR)\n",
    "\n",
    "# Group indices by class\n",
    "class_indices = [[] for _ in range(len(full_dataset.classes))]\n",
    "for idx, (_, label) in enumerate(full_dataset.samples):\n",
    "    class_indices[label].append(idx)\n",
    "\n",
    "# Stratified split\n",
    "train_indices, val_indices = [], []\n",
    "for indices in class_indices:\n",
    "    n_total = len(indices)\n",
    "    n_val = int(n_total * VAL_SPLIT)\n",
    "    random.shuffle(indices)\n",
    "    val_indices.extend(indices[:n_val])\n",
    "    train_indices.extend(indices[n_val:])\n",
    "\n",
    "# Create subsets for DataLoader\n",
    "train_subset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_subset   = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "# Assign transforms to subsets\n",
    "train_subset.dataset.transform = train_transform\n",
    "val_subset.dataset.transform   = val_transform\n",
    "\n",
    "# === TRIPLET DATASET DEFINITION ===\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "        self.targets = [self.subset.dataset.samples[i][1] for i in self.subset.indices]\n",
    "        \n",
    "        # Build dictionary: label → list of indices\n",
    "        self.label_to_indices = {}\n",
    "        for idx, label in zip(self.subset.indices, self.targets):\n",
    "            self.label_to_indices.setdefault(label, []).append(idx)\n",
    "\n",
    "        self.all_indices = self.subset.indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img, anchor_label = self.subset[idx]\n",
    "        \n",
    "        # Select a positive sample\n",
    "        pos_idx = idx\n",
    "        while pos_idx == idx:\n",
    "            pos_idx = random.choice(self.label_to_indices[anchor_label])\n",
    "        positive_img, _ = self.subset[self.subset.indices.index(pos_idx)]\n",
    "        \n",
    "        # Select a negative sample\n",
    "        neg_label = anchor_label\n",
    "        while neg_label == anchor_label:\n",
    "            neg_label = random.choice(list(self.label_to_indices.keys()))\n",
    "        neg_idx = random.choice(self.label_to_indices[neg_label])\n",
    "        negative_img, _ = self.subset[self.subset.indices.index(neg_idx)]\n",
    "\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "# === DATALOADERS ===\n",
    "train_loader = DataLoader(TripletDataset(train_subset), batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(TripletDataset(val_subset),   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4027eb3",
   "metadata": {},
   "source": [
    "3. Model Definition: DINOv2 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba12a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoImageProcessor\n",
    "\n",
    "class DinoV2Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load pretrained model and image processor\n",
    "        self.model = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
    "        \n",
    "        # Freeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze only the last layers and final normalization layer\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if any(x in name for x in [\"encoder.layer.10\", \"encoder.layer.11\", \"layernorm\"]):\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be pre-normalized (handled in the transform pipeline)\n",
    "        outputs = self.model(pixel_values=x)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token\n",
    "        return F.normalize(cls_token, dim=-1)            # Return L2-normalized embedding\n",
    "\n",
    "# Instantiate model\n",
    "model = DinoV2Encoder().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47297ffd",
   "metadata": {},
   "source": [
    "4. Loss Function, Training Loop, and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c09f5dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train: 100%|██████████| 43/43 [00:51<00:00,  1.19s/it]\n",
      "Epoch 1/5 - Val: 100%|██████████| 11/11 [00:09<00:00,  1.12it/s]\n",
      "Val mAP@10 - extracting: 100%|██████████| 11/11 [00:11<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.0109 | Val Loss: 0.0114 | Val mAP@10: 0.9271\n",
      ">> Best model saved (improved mAP@10)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train: 100%|██████████| 43/43 [00:47<00:00,  1.11s/it]\n",
      "Epoch 2/5 - Val: 100%|██████████| 11/11 [00:09<00:00,  1.13it/s]\n",
      "Val mAP@10 - extracting: 100%|██████████| 11/11 [00:11<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.0081 | Val Loss: 0.0037 | Val mAP@10: 0.9456\n",
      ">> Best model saved (improved mAP@10)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train: 100%|██████████| 43/43 [00:47<00:00,  1.11s/it]\n",
      "Epoch 3/5 - Val: 100%|██████████| 11/11 [00:09<00:00,  1.13it/s]\n",
      "Val mAP@10 - extracting: 100%|██████████| 11/11 [00:11<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.0071 | Val Loss: 0.0077 | Val mAP@10: 0.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train: 100%|██████████| 43/43 [00:47<00:00,  1.11s/it]\n",
      "Epoch 4/5 - Val: 100%|██████████| 11/11 [00:09<00:00,  1.12it/s]\n",
      "Val mAP@10 - extracting: 100%|██████████| 11/11 [00:11<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.0095 | Val Loss: 0.0073 | Val mAP@10: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train: 100%|██████████| 43/43 [00:47<00:00,  1.11s/it]\n",
      "Epoch 5/5 - Val: 100%|██████████| 11/11 [00:09<00:00,  1.13it/s]\n",
      "Val mAP@10 - extracting: 100%|██████████| 11/11 [00:11<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.0050 | Val Loss: 0.0050 | Val mAP@10: 0.9358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === LOSS FUNCTION AND OPTIMIZER ===\n",
    "margin = 0.3\n",
    "criterion = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                  lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_val_map10 = 0.0\n",
    "\n",
    "# === VALIDATION METRIC: MEAN AVERAGE PRECISION @10 ===\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def evaluate_map10_on_val(val_subset, model):\n",
    "    model.eval()\n",
    "    loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(loader, desc=\"Val mAP@10 - extracting\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            emb = model(imgs)\n",
    "            features.append(F.normalize(emb, dim=1).cpu())\n",
    "            labels.append(lbls)\n",
    "\n",
    "    features = torch.cat(features, dim=0)  # (N, 768)\n",
    "    labels = torch.cat(labels, dim=0)      # (N,)\n",
    "    N = features.size(0)\n",
    "\n",
    "    sims = cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim=-1)  # (N, N)\n",
    "    sims.masked_fill_(torch.eye(N, dtype=torch.bool), -float('inf'))\n",
    "\n",
    "    ap_total = 0\n",
    "    for i in range(N):\n",
    "        target_label = labels[i]\n",
    "        scores = sims[i]\n",
    "        topk = scores.topk(k=10).indices\n",
    "        hits = (labels[topk] == target_label).float()\n",
    "        precision_at_k = hits.cumsum(0) / torch.arange(1, 11)\n",
    "        ap = (precision_at_k * hits).sum() / hits.sum().clamp(min=1)\n",
    "        ap_total += ap.item()\n",
    "\n",
    "    mean_ap10 = ap_total / N\n",
    "    return mean_ap10\n",
    "\n",
    "# === TRAINING LOOP WITH AMP (AUTOMATIC MIXED PRECISION) ===\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for anchor, positive, negative in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train\"):\n",
    "        anchor   = anchor.to(DEVICE)\n",
    "        positive = positive.to(DEVICE)\n",
    "        negative = negative.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Enable AMP\n",
    "            emb_a = model(anchor)\n",
    "            emb_p = model(positive)\n",
    "            emb_n = model(negative)\n",
    "            loss = criterion(emb_a, emb_p, emb_n)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # === VALIDATION PHASE ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} - Val\"):\n",
    "            anchor   = anchor.to(DEVICE)\n",
    "            positive = positive.to(DEVICE)\n",
    "            negative = negative.to(DEVICE)\n",
    "\n",
    "            with autocast():\n",
    "                emb_a = model(anchor)\n",
    "                emb_p = model(positive)\n",
    "                emb_n = model(negative)\n",
    "                loss = criterion(emb_a, emb_p, emb_n)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # === VALIDATION RETRIEVAL METRIC ===\n",
    "    val_map10 = evaluate_map10_on_val(val_subset, model)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val mAP@10: {val_map10:.4f}\")\n",
    "\n",
    "    # === SAVE BEST MODEL ===\n",
    "    if val_map10 > best_val_map10:\n",
    "        best_val_map10 = val_map10\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\">> Best model saved (improved mAP@10)!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce3f27",
   "metadata": {},
   "source": [
    "5. Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044dcc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (TTA): 100%|██████████| 240/240 [00:19<00:00, 12.62it/s]\n",
      "Extracting features (TTA): 100%|██████████| 15/15 [00:31<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# === RETRIEVAL DATASETS ===\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        self.root = root\n",
    "        self.paths = [os.path.join(root, fname) for fname in os.listdir(root)\n",
    "                      if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(img), self.paths[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "# Load query and gallery datasets\n",
    "query_ds   = SimpleImageDataset(QUERY_DIR, val_transform)\n",
    "gallery_ds = SimpleImageDataset(GALLERY_DIR, val_transform)\n",
    "\n",
    "query_loader   = DataLoader(query_ds, batch_size=1, shuffle=False)\n",
    "gallery_loader = DataLoader(gallery_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# Reload the best model\n",
    "model = DinoV2Encoder().to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# === FEATURE EXTRACTION FUNCTIONS ===\n",
    "def extract_features(dataloader):\n",
    "    features = []\n",
    "    paths = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, img_paths in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            emb = model(imgs)\n",
    "            features.append(emb.cpu())\n",
    "            paths.extend(img_paths)\n",
    "    return torch.cat(features, dim=0), paths\n",
    "\n",
    "def extract_features_with_tta(dataloader):\n",
    "    \"\"\"Applies Test Time Augmentation (TTA) via horizontal flip\"\"\"\n",
    "    features = []\n",
    "    paths = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, img_paths in tqdm(dataloader, desc=\"Extracting features (TTA)\"):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            emb_normal = model(imgs)\n",
    "            imgs_flipped = torch.flip(imgs, dims=[3])  # Horizontal flip\n",
    "            emb_flipped = model(imgs_flipped)\n",
    "            emb_avg = (emb_normal + emb_flipped) / 2\n",
    "            features.append(emb_avg.cpu())\n",
    "            paths.extend(img_paths)\n",
    "    return torch.cat(features, dim=0), paths\n",
    "\n",
    "# Extract features from both query and gallery sets using TTA\n",
    "query_feats, query_paths     = extract_features_with_tta(query_loader)\n",
    "gallery_feats, gallery_paths = extract_features_with_tta(gallery_loader)\n",
    "\n",
    "# === SIMILARITY CALCULATION AND OUTPUT DICTIONARY ===\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "results = {}\n",
    "\n",
    "for q_feat, q_path in zip(query_feats, query_paths):\n",
    "    similarities = cosine_similarity(q_feat.unsqueeze(0), gallery_feats)\n",
    "    topk_indices = torch.topk(similarities, k=TOP_K).indices\n",
    "    topk_paths = [gallery_paths[i] for i in topk_indices]\n",
    "    results[q_path] = topk_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e812ca6",
   "metadata": {},
   "source": [
    "6. Save Retrieval Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f00b64e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval completed. Output saved to 'retrieval_results.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# === FORMAT RESULTS FOR JSON EXPORT ===\n",
    "formatted_results = []\n",
    "\n",
    "for q_path, topk_paths in results.items():\n",
    "    formatted_results.append({\n",
    "        \"filename\": os.path.basename(q_path),\n",
    "        \"samples\": [os.path.basename(p) for p in topk_paths]\n",
    "    })\n",
    "\n",
    "# === SAVE TO JSON FILE ===\n",
    "with open(\"retrieval_results.json\", \"w\") as f:\n",
    "    json.dump(formatted_results, f, indent=2)\n",
    "\n",
    "print(\"✅ Retrieval completed. Output saved to 'retrieval_results.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
