{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebe21e0",
   "metadata": {},
   "source": [
    "1. Load and Preprocess Retrieval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865d842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === LOAD RETRIEVAL RESULTS FROM JSON ===\n",
    "with open(\"retrieval_results.json\", \"r\") as f:\n",
    "    retrieval_data = json.load(f)\n",
    "\n",
    "num_queries = len(retrieval_data)\n",
    "\n",
    "# === HELPER FUNCTION: Extract label from filename ===\n",
    "def label_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts the ground truth label from the filename.\n",
    "    Assumes label is the integer prefix before the first underscore.\n",
    "    E.g., \"12_cat_001.jpg\" â†’ 12\n",
    "    \"\"\"\n",
    "    return int(filename.split(\"_\")[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b465ac9",
   "metadata": {},
   "source": [
    "2. Define Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ee1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AVERAGE PRECISION @k FOR A SINGLE QUERY ===\n",
    "def average_precision_at_k(true_label, retrieved_labels, k):\n",
    "    \"\"\"\n",
    "    Computes Average Precision at rank k for a single query.\n",
    "\n",
    "    Args:\n",
    "        true_label (int): The ground truth label of the query.\n",
    "        retrieved_labels (List[int]): The list of predicted labels.\n",
    "        k (int): Cutoff rank for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Average Precision at k.\n",
    "    \"\"\"\n",
    "    num_hits = 0\n",
    "    precision_values = []\n",
    "    for i in range(min(k, len(retrieved_labels))):\n",
    "        if retrieved_labels[i] == true_label:\n",
    "            num_hits += 1\n",
    "            precision = num_hits / (i + 1)\n",
    "            precision_values.append(precision)\n",
    "\n",
    "    return np.mean(precision_values) if precision_values else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0bd56",
   "metadata": {},
   "source": [
    "3. Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebaa7a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 240/240 [00:00<00:00, 8846.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# === RANK POSITIONS TO EVALUATE ===\n",
    "k_values = [1, 2, 3, 4, 5, 7, 10]\n",
    "\n",
    "# === INITIALIZE METRICS CONTAINERS ===\n",
    "map_scores      = {k: [] for k in k_values}   # Mean Average Precision at k\n",
    "topk_ratios     = {k: [] for k in k_values}   # Precision@k (hit ratio)\n",
    "topk_accuracy   = {k: [] for k in k_values}   # Top-k Accuracy (at least one correct)\n",
    "\n",
    "# === MAIN LOOP OVER RETRIEVAL RESULTS ===\n",
    "for entry in tqdm(retrieval_data, desc=\"Computing metrics\"):\n",
    "    query_file      = entry[\"filename\"]\n",
    "    retrieved_files = entry[\"samples\"]\n",
    "\n",
    "    true_label = label_from_filename(query_file)\n",
    "    retrieved_labels = [label_from_filename(f) for f in retrieved_files]\n",
    "\n",
    "    for k in k_values:\n",
    "        # ----- mAP@k -----\n",
    "        ap = average_precision_at_k(true_label, retrieved_labels, k)\n",
    "        map_scores[k].append(ap)\n",
    "\n",
    "        # ----- Precision@k (hit count / k) -----\n",
    "        hit_count = sum(\n",
    "            1 for i in range(min(k, len(retrieved_labels)))\n",
    "            if retrieved_labels[i] == true_label\n",
    "        )\n",
    "        topk_ratios[k].append(hit_count / k)\n",
    "\n",
    "        # ----- Top-k Accuracy (1 if true_label appears in top-k) -----\n",
    "        hit = int(true_label in retrieved_labels[:k])\n",
    "        topk_accuracy[k].append(hit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19946fcc",
   "metadata": {},
   "source": [
    "4. Print and Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b14de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Mean Average Precision (mAP@k) over 240 queries:\n",
      "mAP@1 : 0.9417\n",
      "mAP@2 : 0.9583\n",
      "mAP@3 : 0.9542\n",
      "mAP@4 : 0.9528\n",
      "mAP@5 : 0.9489\n",
      "mAP@7 : 0.9410\n",
      "mAP@10: 0.9352\n",
      "\n",
      "ðŸ“ˆ Precision@k (hit count / k):\n",
      "P@1 :   0.9417\n",
      "P@2 :   0.9292\n",
      "P@3 :   0.9222\n",
      "P@4 :   0.9156\n",
      "P@5 :   0.9125\n",
      "P@7 :   0.8994\n",
      "P@10:   0.8871\n",
      "\n",
      "ðŸŽ¯ Top-k Accuracy (at least one correct match in top-k):\n",
      "Acc@1 : 0.9417\n",
      "Acc@2 : 0.9750\n",
      "Acc@3 : 0.9750\n",
      "Acc@4 : 0.9833\n",
      "Acc@5 : 0.9833\n",
      "Acc@7 : 0.9875\n",
      "Acc@10: 0.9958\n",
      "\n",
      "ðŸ“Œ Average mAP across all k values: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# === FINAL REPORT =========================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š Mean Average Precision (mAP@k) over {num_queries} queries:\")\n",
    "for k in k_values:\n",
    "    print(f\"mAP@{k:<2}: {np.mean(map_scores[k]):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Precision@k (hit count / k):\")\n",
    "for k in k_values:\n",
    "    print(f\"P@{k:<2}:   {np.mean(topk_ratios[k]):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Top-k Accuracy (at least one correct match in top-k):\")\n",
    "for k in k_values:\n",
    "    print(f\"Acc@{k:<2}: {np.mean(topk_accuracy[k]):.4f}\")\n",
    "\n",
    "# === OPTIONAL: OVERALL AVERAGE OF ALL MAP@K ===\n",
    "overall_mean = np.mean([np.mean(map_scores[k]) for k in k_values])\n",
    "print(f\"\\nðŸ“Œ Average mAP across all k values: {overall_mean:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
